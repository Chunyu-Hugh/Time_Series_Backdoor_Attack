{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 生成示例数据\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 生成正弦波数据加上一些噪声\n",
    "time = np.arange(0, 400, 0.1)\n",
    "data = np.sin(time) + 0.1 * np.random.randn(len(time))\n",
    "\n",
    "# 制作数据集\n",
    "def create_dataset(data, look_back):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        x = data[i:(i + look_back)]\n",
    "        y = data[i + look_back]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "look_back = 20\n",
    "X, Y = create_dataset(data, look_back)\n",
    "X_train, Y_train = torch.FloatTensor(X[:-100]), torch.FloatTensor(Y[:-100])\n",
    "X_test, Y_test = torch.FloatTensor(X[-100:]), torch.FloatTensor(Y[-100:])\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, Y_test), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/anaconda3/envs/chunyu/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers.append(\n",
    "                TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n",
    "                              dilation=dilation_size,\n",
    "                              padding=(kernel_size-1) * dilation_size, dropout=dropout)\n",
    "            )\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.final_layer = nn.Linear(num_channels[-1], 1)  # 添加全连接层以确保输出是一个标量\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # 增加一个维度以适配网络\n",
    "        x = self.network(x)\n",
    "        x = x.squeeze(1)  # 去掉多余的维度\n",
    "        x = self.final_layer(x[:, :, -1])  # 选择最后一个时间点的输出，并通过全连接层\n",
    "        return x\n",
    "\n",
    "model = TemporalConvNet(num_inputs=1, num_channels=[16, 32], kernel_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6239224076271057\n",
      "Epoch 2, Loss: 0.44543954730033875\n",
      "Epoch 3, Loss: 0.8427554368972778\n",
      "Epoch 4, Loss: 0.56889808177948\n",
      "Epoch 5, Loss: 0.371062308549881\n",
      "Epoch 6, Loss: 0.5301308631896973\n",
      "Epoch 7, Loss: 0.356380432844162\n",
      "Epoch 8, Loss: 0.5729440450668335\n",
      "Epoch 9, Loss: 0.5049682259559631\n",
      "Epoch 10, Loss: 0.5576505661010742\n",
      "Epoch 11, Loss: 0.7468507289886475\n",
      "Epoch 12, Loss: 0.5814102292060852\n",
      "Epoch 13, Loss: 0.6765184998512268\n",
      "Epoch 14, Loss: 0.6801705360412598\n",
      "Epoch 15, Loss: 0.505359411239624\n",
      "Epoch 16, Loss: 0.37233367562294006\n",
      "Epoch 17, Loss: 0.5229997634887695\n",
      "Epoch 18, Loss: 0.618018627166748\n",
      "Epoch 19, Loss: 0.6751857995986938\n",
      "Epoch 20, Loss: 0.6104845404624939\n"
     ]
    }
   ],
   "source": [
    "# 设置优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 训练过程\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.47548918426036835\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        output = model(x_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.098836971867469\n",
      "Epoch 2, Loss: 1.0955107865795013\n",
      "Epoch 3, Loss: 1.0936923180857012\n",
      "Epoch 4, Loss: 1.0906382376147854\n",
      "Epoch 5, Loss: 1.0897552467161609\n",
      "Epoch 6, Loss: 1.0851940493429861\n",
      "Epoch 7, Loss: 1.0890301196805892\n",
      "Epoch 8, Loss: 1.0819648888803297\n",
      "Epoch 9, Loss: 1.0796236722700057\n",
      "Epoch 10, Loss: 1.0767486056973856\n",
      "Epoch 11, Loss: 1.0711935835499917\n",
      "Epoch 12, Loss: 1.0758843768027522\n",
      "Epoch 13, Loss: 1.071782269785481\n",
      "Epoch 14, Loss: 1.068077189307059\n",
      "Epoch 15, Loss: 1.0706683454975006\n",
      "Epoch 16, Loss: 1.0618713075114834\n",
      "Epoch 17, Loss: 1.0632517183980634\n",
      "Epoch 18, Loss: 1.058184479513476\n",
      "Epoch 19, Loss: 1.0603989247352845\n",
      "Epoch 20, Loss: 1.0618263598411315\n",
      "Epoch 21, Loss: 1.0455755745210955\n",
      "Epoch 22, Loss: 1.0392987785800811\n",
      "Epoch 23, Loss: 1.0483193359067362\n",
      "Epoch 24, Loss: 1.0405565557941314\n",
      "Epoch 25, Loss: 1.0351342693451913\n",
      "Epoch 26, Loss: 1.0455350068307692\n",
      "Epoch 27, Loss: 1.0356935897181112\n",
      "Epoch 28, Loss: 1.0236457470924623\n",
      "Epoch 29, Loss: 1.0353397034829663\n",
      "Epoch 30, Loss: 1.0249822447376866\n",
      "Epoch 31, Loss: 1.0195986340122838\n",
      "Epoch 32, Loss: 1.0209382234081146\n",
      "Epoch 33, Loss: 1.0126975813219625\n",
      "Epoch 34, Loss: 1.0190491637875956\n",
      "Epoch 35, Loss: 1.0113427985099055\n",
      "Epoch 36, Loss: 0.9982500845386136\n",
      "Epoch 37, Loss: 1.0059662499735433\n",
      "Epoch 38, Loss: 1.0028233047454589\n",
      "Epoch 39, Loss: 0.9946832522269218\n",
      "Epoch 40, Loss: 0.9920491960740858\n",
      "Epoch 41, Loss: 0.9966657507804132\n",
      "Epoch 42, Loss: 0.9930168274910219\n",
      "Epoch 43, Loss: 1.0018075031618918\n",
      "Epoch 44, Loss: 0.9825169617129911\n",
      "Epoch 45, Loss: 0.978530966466473\n",
      "Epoch 46, Loss: 0.9880867465849845\n",
      "Epoch 47, Loss: 0.9674786848406638\n",
      "Epoch 48, Loss: 0.9655941455594955\n",
      "Epoch 49, Loss: 0.9634150708875349\n",
      "Epoch 50, Loss: 0.9851146167324435\n",
      "Epoch 51, Loss: 0.9737935758406117\n",
      "Epoch 52, Loss: 0.9827826388420597\n",
      "Epoch 53, Loss: 0.968645784162706\n",
      "Epoch 54, Loss: 0.9722643436924103\n",
      "Epoch 55, Loss: 0.9383539480547751\n",
      "Epoch 56, Loss: 0.9663725706838793\n",
      "Epoch 57, Loss: 0.9496674826068263\n",
      "Epoch 58, Loss: 0.9468814211506997\n",
      "Epoch 59, Loss: 0.9368737243836925\n",
      "Epoch 60, Loss: 0.9293022424943985\n",
      "Epoch 61, Loss: 0.9480331386289289\n",
      "Epoch 62, Loss: 0.9543678472118993\n",
      "Epoch 63, Loss: 0.940650305440349\n",
      "Epoch 64, Loss: 0.9443893586435625\n",
      "Epoch 65, Loss: 0.9342647502499242\n",
      "Epoch 66, Loss: 0.9324297789604433\n",
      "Epoch 67, Loss: 0.9257557007574266\n",
      "Epoch 68, Loss: 0.9494020458190672\n",
      "Epoch 69, Loss: 0.9426421715367225\n",
      "Epoch 70, Loss: 0.9082954045264952\n",
      "Epoch 71, Loss: 0.9118938003816912\n",
      "Epoch 72, Loss: 0.9324088461937443\n",
      "Epoch 73, Loss: 0.9256091944632991\n",
      "Epoch 74, Loss: 0.9194627750304437\n",
      "Epoch 75, Loss: 0.9040399174536428\n",
      "Epoch 76, Loss: 0.93397500822621\n",
      "Epoch 77, Loss: 0.9409519549339048\n",
      "Epoch 78, Loss: 0.9090236636900133\n",
      "Epoch 79, Loss: 0.904558983541304\n",
      "Epoch 80, Loss: 0.918730124350517\n",
      "Epoch 81, Loss: 0.9048040778406204\n",
      "Epoch 82, Loss: 0.9112413852445541\n",
      "Epoch 83, Loss: 0.9100147985642956\n",
      "Epoch 84, Loss: 0.9031028516830937\n",
      "Epoch 85, Loss: 0.905762707033465\n",
      "Epoch 86, Loss: 0.8840247161926762\n",
      "Epoch 87, Loss: 0.8837334225254674\n",
      "Epoch 88, Loss: 0.8844575382048084\n",
      "Epoch 89, Loss: 0.9089478658091638\n",
      "Epoch 90, Loss: 0.9098772118168492\n",
      "Epoch 91, Loss: 0.9035785775030812\n",
      "Epoch 92, Loss: 0.9150039534414968\n",
      "Epoch 93, Loss: 0.8962291844429509\n",
      "Epoch 94, Loss: 0.8806925127583165\n",
      "Epoch 95, Loss: 0.8880582144183498\n",
      "Epoch 96, Loss: 0.8679791342827582\n",
      "Epoch 97, Loss: 0.8760227099541695\n",
      "Epoch 98, Loss: 0.871408752856716\n",
      "Epoch 99, Loss: 0.8747675457308369\n",
      "Epoch 100, Loss: 0.8777927064126537\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch\n",
    "\n",
    "# 生成示例数据\n",
    "def generate_data(sequence_length=1000, num_classes=3):\n",
    "    X = torch.randn(sequence_length, 1)  # 随机特征，每个特征维度为1\n",
    "    Y = torch.randint(0, num_classes, (sequence_length,))  # 随机标签，标签范围[0, num_classes)\n",
    "    return X, Y\n",
    "\n",
    "# 创建数据集\n",
    "def create_dataset(X, Y, look_back=20):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(X) - look_back):\n",
    "        a = X[i:(i + look_back)].reshape(-1)  # 重塑每个窗口为一个连续的数组\n",
    "        dataX.append(a)\n",
    "        dataY.append(Y[i + look_back])\n",
    "    return torch.stack(dataX), torch.stack(dataY)\n",
    "\n",
    "# 数据参数\n",
    "sequence_length = 1000\n",
    "look_back = 20\n",
    "num_classes = 3\n",
    "\n",
    "# 生成并处理数据\n",
    "X, Y = generate_data(sequence_length, num_classes)\n",
    "X, Y = create_dataset(X, Y, look_back)\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(X.unsqueeze(1), Y)  # 增加一个维度用于表示单通道输入\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 定义模型\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                      stride=stride, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return self.chomp1(x)\n",
    "# Let's assume:\n",
    "num_channels_last_conv_layer = 32  # Or whatever your last conv layer's output channels are\n",
    "look_back = 20  # Assuming the output sequence length remains the same as input\n",
    "num_classes = 3\n",
    "\n",
    "# Define your model:\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, num_classes, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        self.layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            self.layers.append(\n",
    "                TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n",
    "                              dilation=dilation_size,\n",
    "                              padding=(kernel_size-1) * dilation_size, dropout=dropout)\n",
    "            )\n",
    "        self.network = nn.Sequential(*self.layers)\n",
    "        self.final_layer = nn.Linear(832, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.unsqueeze(1)  # Ensure the channel dimension is correct\n",
    "        x = self.network(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten all dimensions except the batch\n",
    "        # print(\"Dimension before Linear layer:\", x.shape)  # This will tell you the actual dimension\n",
    "\n",
    "        return self.final_layer(x)\n",
    "\n",
    "model = TemporalConvNet(num_inputs=1, num_channels=[16, 32], num_classes=3, kernel_size=2)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.2410, Accuracy: 32.78%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.2409824927647908, 32.77777777777778)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设已经有了X_test和Y_test\n",
    "# 这里用同样的方法创建测试数据集\n",
    "X_test, Y_test = generate_data(sequence_length=200, num_classes=num_classes)  # 生成更少的数据用于测试\n",
    "X_test, Y_test = create_dataset(X_test, Y_test, look_back)\n",
    "\n",
    "test_dataset = TensorDataset(X_test.unsqueeze(1), Y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # 在评估过程中不计算梯度\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "evaluate_model(model, test_loader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunyu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
